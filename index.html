<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Runxuan (Jerry) Wang</title>

    <meta name="author" content="Runxuan (Jerry) Wang">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">

  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Runxuan (Jerry) Wang
                </p>
                <p>I'm a first-year robotics master student at <a href="https://www.seas.upenn.edu/"></a>University of Pennsylvania</a>. Prior to UPenn, I received my B.S. in Computer Engineering from <a href="https://illinois.edu/">University of Illinois at Urbana-Champaign</a>. At UIUC, I was an undergraduate research assistant at <a href="https://yunzhuli.github.io">Robotic Perception, Interaction, and Learning Lab</a> with <a href="https://yunzhuli.github.io">Prof. Yunzhu Li</a> and <a href="https://thehcalab.web.illinois.edu">Human Centered Autonomy Lab</a> advised by <a href="https://ece.illinois.edu/about/directory/faculty/krdc">Prof. Katie Driggs-Campbell</a>.
                </p>
                <p>
                  My current research interest is in robot learning and perception. My research goal is to bridge the gap of robots between simulation and the real world. At UIUC, I was fortunate to lead the embedded system team of <a href="https://github.com/illini-robomaster">Illini RoboMaster</a> across multiple competitions.
                </p>
                <p style="text-align:center">
                  <a href="mailto:runxuan@seas.upenn.edu">Email</a> &nbsp;/&nbsp;
                  <a href="data/jerrywang_CV.pdf">CV</a> &nbsp;/&nbsp;
                  <!-- <a href="data/JonBarron-bio.txt">Bio</a> &nbsp;/&nbsp; -->
                  <!-- <a href="https://scholar.google.com/citations?hl=en&user=jktWnL8AAAAJ">Scholar</a> &nbsp;/&nbsp; -->
                  <a href="https://www.linkedin.com/in/runxuan-wang/">Linkedin</a> &nbsp;/&nbsp;
                  <a href="https://github.com/jerrywrx">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:40%;max-width:40%">
                <a href="images/portrait.jpg"><img style="width:100%;max-width:100%;object-fit: cover; border-radius: 50%;" alt="profile photo" src="images/portrait.jpg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>



          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <!-- <p>
                  I'm interested in computer vision, deep learning, generative AI, and image processing. Most of my research is about inferring the physical world (shape, motion, color, light, etc) from images, usually with radiance fields. Some papers are <span class="highlight">highlighted</span>.
                </p> -->
              </td>
            </tr>
          </tbody></table>




          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>

          <tr onmouseout="spa_stop()" onmouseover="spa_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='spa_image'><video  width=100% muted autoplay loop>
                <source src="images/spa.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/spa.png' width=100%>
              </div>
              <script type="text/javascript">
                function spa_start() {
                  document.getElementById('spa_image').style.opacity = "1";
                }

                function spa_stop() {
                  document.getElementById('spa_image').style.opacity = "0";
                }
                spa_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <span class="papertitle">Sim-to-Real Adaptation with Graph Networks for Granular Objects Scooping Tasks</span>
              <br>

              <a href="https://www.linkedin.com/in/kaiwen-hong-524520141/?locale=en_US">Kaiwen Hong</a>,
              <a href="https://www.linkedin.com/in/haonan-chen-7a4339153/">Haonan Chen*</a>,
              <strong>Runxuan Wang*</strong>,
              <a href="https://www.linkedin.com/in/kaylanwang/">Kaylan Wang*</a>,
              <a href="https://robo-alex.github.io">Mingtong Zhang</a>,
              <a href="https://shuijing725.github.io">Shuijing Liu</a>,<br>
              <a href="https://yunzhuli.github.io">Yunzhu Li</a>,
              <a href="https://ece.illinois.edu/about/directory/faculty/krdc">Katherine Driggs-Campbell</a>

              <br>
              <em>arXiv</em>, 2024
              <br>
              <a href="https://nerf-casting.github.io/">project page</a>
              /
              <a href="https://arxiv.org/abs/2405.14871">arXiv</a>
              <p></p>
              <p>
              A GNN-based dynamics model combined with MCTS lets robots efficiently scoop granular objects from containers, bridging the sim-to-real gap by adapting simulation-trained models with minimal real-world data.
              </p>
            </td>
          </tr>


          <tr onmouseout="dragon_stop()" onmouseover="dragon_start()">
            <td style="padding:20px;width:25%;vertical-align:middle">
              <div class="one">
                <div class="two" id='dragon_image'><video  width=100% muted autoplay loop>
                <source src="images/flash_cache.mp4" type="video/mp4">
                Your browser does not support the video tag.
                </video></div>
                <img src='images/dragon.png' width=100%>
              </div>
              <script type="text/javascript">
                function dragon_start() {
                  document.getElementById('dragon_image').style.opacity = "1";
                }

                function dragon_stop() {
                  document.getElementById('dragon_image').style.opacity = "0";
                }
                dragon_stop()
              </script>
            </td>
            <td style="padding:20px;width:75%;vertical-align:middle">
              <span class="papertitle">DRAGON: A Dialogue-Based Robot for Assistive Navigation with Visual Language Grounding</span>
              <br>
              <a href="https://shuijing725.github.io">Shuijing Liu</a>,
              <a href="https://aamzhas.github.io">Aamir Hasan</a>,
              <a href="https://www.linkedin.com/in/kaiwen-hong-524520141/?locale=en_US">Kaiwen Hong</a>,
              <strong>Runxuan Wang</strong>,
              <a href="https://sites.google.com/site/changpeixin/home">Peixin Chang</a>,
              Zachery Mizrachi,
              Justin Lin,
              <a href="https://scholar.google.com/citations?user=83a8tp0AAAAJ&hl=en">D. Livingston McPherson</a>,
              <a href="https://lifehome.ahs.illinois.edu/rogers/">Wendy A. Rogers</a>,
              <a href="https://ece.illinois.edu/about/directory/faculty/krdc">Katherine Driggs-Campbell</a>
              <br>
              <em>IEEE Robotics and Automation Letters (RA-L)</em>, 2024
              <br>
              <a href="https://sites.google.com/view/dragon-wayfinding">project page</a>
              /
              <a href="https://www.youtube.com/playlist?list=PLL4IPhbfiY3YkITpyLjeroak_wBn151pn">video</a>
              /
              <a href="https://github.com/Shuijing725/Dragon_Wayfinding">code</a>
              /
              <a href="https://arxiv.org/abs/2307.06924">paper</a>
              <p></p>
              <p>
                A dialogue-based robot integrates CLIP-powered visual-language grounding with autonomous navigation to assist visually impaired users, guiding them to desired landmarks and providing real-time environmental descriptions through natural, unconstrained speech interactions.
              </p>
            </td>
          </tr>

          </tbody></table>



					<table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
						<tr>
						<td style="padding:10px;width:100%;vertical-align:middle">
							<h2>Projects</h2>
							<!-- <p>
								I'm interested in computer vision, deep learning, generative AI, and image processing. Most of my research is about inferring the physical world (shape, motion, color, light, etc) from images, usually with radiance fields. Some papers are <span class="highlight">highlighted</span>.
							</p> -->
						</td>
            </tr>
          </tbody></table>


          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h3>Autonomous “Sentry” Robot for RoboMaster Competition</h3>
                <p>
                  In this project, I developed the complete software stack for an autonomous omnidirectional robot using ROS and FreeRTOS, enabling smooth navigation across various environments through FAST-LIO SLAM and TEB planner, utilizing a MID360 3D LiDAR.
                </p>
              </td>
            </tr>
          </tbody></table>

          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:20px;width:33%;text-align:center">
                <video width="100%" height="auto" autoplay loop muted playsinline>
                  <source src="sentry1.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
                <p>Navigation through waypoints</p>
              </td>
              <td style="padding:20px;width:33%;text-align:center">
                <video width="100%" height="auto" autoplay loop muted playsinline>
                  <source src="sentry2.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
                <p>Navigation in "defence" mode</p>
              </td>
              <td style="padding:20px;width:33%;text-align:center">
                <video width="100%" height="auto" autoplay loop muted playsinline>
                  <source src="sentry3.mp4" type="video/mp4">
                  Your browser does not support the video tag.
                </video>
                <p>Avoiding dynamic obstacles</p>
              </td>
            </tr>
          </tbody></table>



          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Source code from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>'s website.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
